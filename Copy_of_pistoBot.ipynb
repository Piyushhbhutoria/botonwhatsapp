{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of pistoBot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Piyushhbhutoria/botonwhatsapp/blob/master/Copy_of_pistoBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMBML9lTaLva"
      },
      "source": [
        "# **ü§ñ pistoBot**\n",
        "\n",
        "> Create an AI that (try) to chat like you.<br>\n",
        "> by [Simone Guardati](https://www.linkedin.com/in/simone-guardati/)\n",
        "\n",
        "\n",
        "## ü•ú In a nutshell\n",
        "1. Get your whatsapp and telegram data\n",
        "2. Parse it to get a ML-like dataset\n",
        "3. Train a GTP2 model\n",
        "4. Chat with the model\n",
        "\n",
        "**üîó Resources**\n",
        "- Chat parser - [github](https://github.com/GuardatiSimone/messaging-chat-parser)\n",
        "- pistoBot - [github](https://github.com/GuardatiSimone/pistoBot)\n",
        "- pistoBot website - [link](https://guardatisimone.github.io/pistoBot-website/)\n",
        "\n",
        "<br>\n",
        "\n",
        "**‚ö†Ô∏è Warning**\n",
        "- It's always better not to run random scripts on personal information (like personal chat messages).\n",
        "- I guarantee there is no double end, but you can always check (and use) the native code: <br>\n",
        "[messaging-chat-parser](https://github.com/GuardatiSimone/messaging-chat-parser) and [pistobot](https://github.com/GuardatiSimone/pistoBot)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GLofGtbgR5c"
      },
      "source": [
        "------\n",
        "## 0Ô∏è‚É£ Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zasps9vfHk0",
        "outputId": "8d739bc4-3051-4309-c86f-7d1a50909ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!nvidia-smi # p100 suggested"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Oct 26 19:37:02 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P0    63W / 149W |      0MiB / 11441MiB |     55%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clix8N59cqqL",
        "outputId": "68d25707-5851-43c1-d0cc-e74a3abb94ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "!git clone --quiet https://github.com/GuardatiSimone/messaging-chat-parser.git\n",
        "!pip install -q -r messaging-chat-parser/requirements.txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC2uQnjRnBM5"
      },
      "source": [
        "!git clone --quiet https://github.com/GuardatiSimone/pistoBot.git"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qEkk-YZbDN_"
      },
      "source": [
        "-----\n",
        "## 1Ô∏è‚É£ Get the data\n",
        "> Get your chat from whatsapp and telegram and upload them under `./messaging-chat-parser/data/chat_raw/` notebook folder\n",
        "\n",
        "\n",
        "- **WhatsApp**\n",
        "    - _.txt_ files exported from one or more chat - [how](https://faq.whatsapp.com/en/android/23756533/) (under \"Export chat history\" section)\n",
        "        - don't export images\n",
        "        - don't export group chats\n",
        "    - place all txt files in `./messaging-chat-parser/data/chat_raw/whatsapp/*.txt`\n",
        "- **Telegram** \n",
        "    - _.json_ with the telegram dump - [how](https://telegram.org/blog/export-and-more)\n",
        "        - don't export images\n",
        "        - choose \"machine-readable JSON\"\n",
        "    - copy and rename the json file in `./messaging-chat-parser/data/chat_raw/telegram/telegram_dump.json`\n",
        "\n",
        "\n",
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq_6S7lCemLW"
      },
      "source": [
        "## 2Ô∏è‚É£ Parse the data\n",
        "\n",
        "**Whatsapp**\n",
        "- Set the following variable `whatsapp_user_name`\n",
        "    - Get it from one of the WhatsApp chat exported text. \n",
        "    <br> e.g. from one line: <br> \n",
        "     _12/12/19, 08:40 - `whatsapp_user_name`: bla bla bla_ \n",
        "- Datetime:\n",
        "    - WhatsApp and Telegram have two different ways to manage the datetime.\n",
        "    - Here are listed the two format, with Italian default values, if your data have different formats, change accordingly the next line values\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHSOoxejg8s4"
      },
      "source": [
        "whatsapp_user_name = \"Piyushh\"\n",
        "whatsapp_datetime_format = \"%m/%d/%y, %H:%M %p\"\n",
        "telegram_datetime_format = \"%Y-%m-%dT%H:%M:%S\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3_N2WUso_XI",
        "outputId": "aef0d9b8-04e3-4fb0-bd57-b309069d3aba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "# Whatsapp\n",
        "print(\"> [WHATSAPP] start parsing...\")\n",
        "assert whatsapp_user_name is not None, \"[!] Whatsapp user name not setted\"\n",
        "!cd messaging-chat-parser && python ./src/whatsapp_parser.py --session_token \"<|endoftext|>\" --user_name $whatsapp_user_name --time_format \"$whatsapp_datetime_format\"\n",
        "print(\"> [WHATSAPP] parsing completed!\\n\\n\")\n",
        "print(\"----------------------------------\")\n",
        "\n",
        "# Telegram\n",
        "print(\"> [TELEGRAM] start parsing...\")\n",
        "assert os.path.exists(\"./messaging-chat-parser/data/chat_raw/telegram/telegram_dump.json\") is not None, \"[!] `telegram_dump.json` not loaded\"\n",
        "!cd messaging-chat-parser && python ./src/telegram_parser.py --session_token \"<|endoftext|>\" --time_format \"$telegram_datetime_format\"\n",
        "print(\"> [TELEGRAM] parsing completed!\\n\\n\")\n",
        "print(\"----------------------------------\")\n",
        "\n",
        "# Join Telegram and Whatsapp data\n",
        "!cd messaging-chat-parser/ && python ./src/joiner.py\n",
        "training_data_lines = sum(1 for line in open('./messaging-chat-parser/data/chat_parsed/all-messages.txt'))\n",
        "print(f\"> [PARSER] Training file lines: {training_data_lines}\")\n",
        "print(\"----------------------------------\")\n",
        "\n",
        "# Check data size\n",
        "if training_data_lines < 100000:\n",
        "    print(f\"[WARNING] attention insufficient training data ({training_data_lines} < 100K), it is recommended to export more chats\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> [WHATSAPP] start parsing...\n",
            "[whatsapp_parser.py][INFO]: WA_STOP_WORDS:['https', '<Media omessi>', '<Media omitted>', 'www']\n",
            "[whatsapp_parser.py][INFO]: Found 12 txt files in `./data/chat_raw/whatsapp/` folder: ['./data/chat_raw/whatsapp/WhatsApp Chat with Navin ECell (1).txt', './data/chat_raw/whatsapp/WhatsApp Chat with Tanvi.txt', './data/chat_raw/whatsapp/WhatsApp Chat with Aseem Sarvapriya.txt', './data/chat_raw/whatsapp/WhatsApp Chat with Shreya Khaitan.txt', './data/chat_raw/whatsapp/WhatsApp Chat with Shubhra PI.txt', './data/chat_raw/whatsapp/WhatsApp Chat with Manisha Chandak.txt', './data/chat_raw/whatsapp/WhatsApp Chat with Saharsh Modi.txt', './data/chat_raw/whatsapp/WhatsApp Chat with Tarun Sethia.txt', './data/chat_raw/whatsapp/WhatsApp Chat with Navin ECell.txt', './data/chat_raw/whatsapp/WhatsApp Chat with Chetan Chhalani.txt', './data/chat_raw/whatsapp/WhatsApp Chat with Pratiksha ECell.txt', './data/chat_raw/whatsapp/WhatsApp Chat with Jyotsna.txt']\n",
            "[whatsapp_parser.py][INFO]: Found 7 invalid lines in ./data/chat_raw/whatsapp/WhatsApp Chat with Navin ECell (1).txt\n",
            "[whatsapp_parser.py][INFO]: Found 59 invalid lines in ./data/chat_raw/whatsapp/WhatsApp Chat with Tanvi.txt\n",
            "[whatsapp_parser.py][INFO]: Found 1290 invalid lines in ./data/chat_raw/whatsapp/WhatsApp Chat with Aseem Sarvapriya.txt\n",
            "[whatsapp_parser.py][INFO]: Found 225 invalid lines in ./data/chat_raw/whatsapp/WhatsApp Chat with Shreya Khaitan.txt\n",
            "[whatsapp_parser.py][INFO]: Found 1584 invalid lines in ./data/chat_raw/whatsapp/WhatsApp Chat with Shubhra PI.txt\n",
            "[whatsapp_parser.py][INFO]: Found 671 invalid lines in ./data/chat_raw/whatsapp/WhatsApp Chat with Manisha Chandak.txt\n",
            "[whatsapp_parser.py][INFO]: Found 1221 invalid lines in ./data/chat_raw/whatsapp/WhatsApp Chat with Saharsh Modi.txt\n",
            "[whatsapp_parser.py][INFO]: Found 314 invalid lines in ./data/chat_raw/whatsapp/WhatsApp Chat with Tarun Sethia.txt\n",
            "[whatsapp_parser.py][INFO]: Found 263 invalid lines in ./data/chat_raw/whatsapp/WhatsApp Chat with Navin ECell.txt\n",
            "[whatsapp_parser.py][INFO]: Found 3178 invalid lines in ./data/chat_raw/whatsapp/WhatsApp Chat with Chetan Chhalani.txt\n",
            "[whatsapp_parser.py][INFO]: Found 424 invalid lines in ./data/chat_raw/whatsapp/WhatsApp Chat with Pratiksha ECell.txt\n",
            "[whatsapp_parser.py][INFO]: Found 267 invalid lines in ./data/chat_raw/whatsapp/WhatsApp Chat with Jyotsna.txt\n",
            "[whatsapp_parser.py][INFO]: Saving ./data/chat_parsed/wa-chats.txt\n",
            "> [WHATSAPP] parsing completed!\n",
            "\n",
            "\n",
            "----------------------------------\n",
            "> [TELEGRAM] start parsing...\n",
            "[telegram_parser.py][INFO]: Loading telegram user data at ./data/chat_raw/telegram/telegram_dump.json...\n",
            "[telegram_parser.py][INFO]: Start parsing telegram messages...\n",
            "  0% 0/2 [00:00<?, ?it/s][telegram_parser.py][INFO]: Processing chat with `Telegram`\n",
            "[telegram_parser.py][INFO]: Processing chat with `Kushagra`\n",
            "100% 2/2 [00:00<00:00, 615.72it/s]\n",
            "[telegram_parser.py][INFO]: N¬∞ 13 invalid lines found, top 5: ['[STOP_WORD] 4295744296 - [\"Telegram Desktop for your Mac or Windows PC graduates to version 1.0 today, adding a fabulous new design and support for custom themes!\\\\n\\\\n- Our native stand-alone desktop app was first released in 2013 and works on Windows, macOS, and Linux. \\\\n- Telegram Desktop doesn\\'t require a mobile phone to work.\\\\n- It syncs existing messages from your phone with your computer using Telegram‚Äôs encrypted cloud.\\\\n\\\\nRead more about this update: \\\\n\", {\\'type\\': \\'link\\', \\'text\\': \\'https://telegram.org/blog/desktop-1-0\\'}, \\'\\\\n\\\\nAnd get Telegram Desktop now:\\\\n\\', {\\'type\\': \\'link\\', \\'text\\': \\'https://desktop.telegram.org\\'}, \\'\\']', \"[STOP_WORD] 4295744296 - ['Telegram has reached 200 million monthly active users. Thanks for spreading the word! \\\\n\\\\n', {'type': 'link', 'text': 'https://telegram.org/blog/200-million'}]\", '[STOP_WORD] 4295744296 - [{\\'type\\': \\'bold\\', \\'text\\': \\'Login code:\\'}, \\' 19906. Do \\', {\\'type\\': \\'bold\\', \\'text\\': \\'not\\'}, \" give this code to anyone, even if they say they are from Telegram!\\\\n\\\\nThis code can be used to log in to your Telegram account. We never ask it for anything else.\\\\n\\\\nIf you didn\\'t request this code by trying to log in on another device, simply ignore this message.\"]', '[STOP_WORD] 4295744296 - [{\\'type\\': \\'bold\\', \\'text\\': \\'Login code:\\'}, \\' 59655. Do \\', {\\'type\\': \\'bold\\', \\'text\\': \\'not\\'}, \" give this code to anyone, even if they say they are from Telegram!\\\\n\\\\nThis code can be used to log in to your Telegram account. We never ask it for anything else.\\\\n\\\\nIf you didn\\'t request this code by trying to log in on another device, simply ignore this message.\"]', '[STOP_WORD] 4295744296 - [{\\'type\\': \\'bold\\', \\'text\\': \\'Login code:\\'}, \\' 84091. Do \\', {\\'type\\': \\'bold\\', \\'text\\': \\'not\\'}, \" give this code to anyone, even if they say they are from Telegram!\\\\n\\\\nThis code can be used to log in to your Telegram account. We never ask it for anything else.\\\\n\\\\nIf you didn\\'t request this code by trying to log in on another device, simply ignore this message.\"]']\n",
            "[telegram_parser.py][INFO]: Saving 12^ telegram messages...\n",
            "> [TELEGRAM] parsing completed!\n",
            "\n",
            "\n",
            "----------------------------------\n",
            "[joiner.py][INFO]: files_directory:./data/chat_parsed/ - files_name:['telegram-chats.txt', 'wa-chats.txt']\n",
            "[joiner.py][INFO]: N¬∞ User messages - 34157 messages found. Saving at: ./data/chat_parsed/user-messages.txt\n",
            "[joiner.py][INFO]: N¬∞ Chat messages - 72206 messages found. Saving at: ./data/chat_parsed/all-messages.txt\n",
            "[joiner.py][INFO]: Joiner finished\n",
            "> [PARSER] Training file lines: 72206\n",
            "----------------------------------\n",
            "[WARNING] attention insufficient training data (72206 < 100K), it is recommended to export more chats\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDiyALCfsOwq"
      },
      "source": [
        "------\n",
        "## 3Ô∏è‚É£ Train a GTP2 model\n",
        "\n",
        "- ‚è≥ The following cell could take **up to 10 hours**\n",
        "    - An estimation of the total time will be prompted\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwHVtFvPfBXg",
        "outputId": "3c0ddabe-dd6e-43ac-c378-70689a8e4b11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cp ./messaging-chat-parser/data/chat_parsed/all-messages.txt ./pistoBot/data/inputs/chat_parsed/all-messages-endoftext.txt\n",
        "!cd pistoBot/colab/ && bash run_training.sh gpt2-scratch"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing common requirements...\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 276kB 2.7MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460kB 8.6MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 409kB 12.9MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.7MB 15.5MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 7.1MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 409kB 19.8MB/s \n",
            "\u001b[?25h  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for twilio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "[gpt2-scratch model chosen]\n",
            "Installing requirements...\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 256kB 2.7MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 573kB 8.5MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 829kB 13.5MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 675kB 19.7MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81kB 7.5MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1MB 18.9MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 890kB 33.8MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.8MB 40.6MB/s \n",
            "\u001b[?25h  Building wheel for aitextgen (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Training model...\n",
            "2020-10-26 15:59:36.917045: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "WARNING:root:Keys file at ./data/inputs/personal/my-keys.txt not found\n",
            "INFO:root:Training tokenizer...\n",
            "[00:00:00] Reading files                            ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë                   0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files                            ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë                   3\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files                            ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë                   7\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files                            ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë                  11\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files                            ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë                  15\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files                            ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë                  19\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files                            ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë                  24\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files                            ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë                  29\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files                            ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë                  33\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files                            ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë                  37\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files                            ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë                  41\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files                            ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë                  45\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files                            ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë                  49\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Reading files                            ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë                  53\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files                            ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë                  58\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files                            ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë                  63\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files                            ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë                  67\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files                            ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë                  71\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files                            ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë                  75\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files                            ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë                  79\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files                            ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë                  83\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files                            ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë                  87\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files                            ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë                  91\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files                            ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë                  95\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files                            ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë                  99\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Reading files                            ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 100\n",
            "[00:00:00] Tokenize words                           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0        /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 5499     /    21416\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë 10766    /    21416\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 16605    /    21416\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 21416    /    21416\n",
            "\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 21416    /    21416\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 21416    /    21416\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 21416    /    21416\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 21416    /    21416\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 21416    /    21416\n",
            "\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 21416    /    21416\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 21416    /     5000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 21416    /     5000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 21416    /     5000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 24       /     5000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 43       /     5000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 45       /     5000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 75       /     5000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 109      /     5000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 354      /     5000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 1067     /     5000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë 2184     /     5000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë 3414     /     5000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 4718     /     5000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 4743     /     4743\n",
            "\n",
            "INFO:aitextgen.tokenizers:Saving aitextgen-vocab.json and aitextgen-merges.txt to ./data/models_trained/03_gpt2scratch_20201026155939. You will need both files to build the GPT2Tokenizer.\n",
            "INFO:root:Training tokenizer completed!\n",
            "INFO:root:Training model...\n",
            "INFO:aitextgen:Constructing GPT-2 model from provided config.\n",
            "INFO:aitextgen:Using a custom tokenizer.\n",
            "  0% 0/72206 [00:00<?, ?it/s]INFO:aitextgen.TokenDataset:Encoding 72,206 sets of tokens from ./data/inputs/chat_parsed/all-messages-endoftext.txt.\n",
            "100% 72206/72206 [00:02<00:00, 34598.02it/s]\n",
            "GPU available: True, used: True\n",
            "INFO:lightning:GPU available: True, used: True\n",
            "No environment variable for node rank defined. Set as 0.\n",
            "WARNING:lightning:No environment variable for node rank defined. Set as 0.\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[1m1,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m2,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m3,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m4,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m5,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m5,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "                                                                                       \n",
            "[others] Bhai\n",
            "[others] Sunnn\n",
            "[others] Call kar\n",
            "[others] Tere madl tak kuch\n",
            "[others]\n",
            "==========\n",
            "\u001b[1m6,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m7,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m8,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m9,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m10,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m10,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "                                                                                          \n",
            "[me] Bhai\n",
            "[me] 30 min ruk\n",
            "[others] Jyoti BECon ka verifm started the sus\n",
            "==========\n",
            "\u001b[1m11,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m12,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m13,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m14,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m15,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m15,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "                                                                                          \n",
            "[me] Hey\n",
            "[me] Hi\n",
            "[others] Bol bhai bol hey hi ke aage bhi\n",
            "[me] Kkrh\n",
            "[others]\n",
            "==========\n",
            "\u001b[1m16,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m17,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m18,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m19,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m20,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m20,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "                                                                                          \n",
            "[others] NHI khel raha?\n",
            "\n",
            "==========\n",
            "\u001b[1m21,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m22,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m23,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m24,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m25,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m25,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "                                                                                        \n",
            "[me] Sun\n",
            "[me] Tu Vellore kyu aaya hai waise?\n",
            "[others] Aise hi ghoomne aaya tha bhai\n",
            "[me\n",
            "==========\n",
            "\u001b[1m26,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m27,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m28,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m29,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m30,000 steps reached: saving model to /./data/models_trained/03_gpt2scratch_20201026155939\u001b[0m\n",
            "\u001b[1m30,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "                                                                                        \n",
            "[me] Bhai kal Friday haiüòÇ\n",
            "[me] 27th\n",
            "[others] Haan\n",
            "[others] To?\n",
            "[me] Plan\n",
            "==========\n",
            "Loss: 0.409 ‚Äî Avg: 0.408 ‚Äî GPU Mem: 3627 MB: 100% 30000/30000 [3:36:29<00:00,  2.31it/s]INFO:aitextgen:Saving trained model pytorch_model.bin to /./data/models_trained/03_gpt2scratch_20201026155939\n",
            "INFO:root:Training completed!\n",
            "INFO:root:Generation starting...\n",
            "INFO:aitextgen:Generating 100 texts to ./data/models_trained/03_gpt2scratch_20201026155939/generation/20201026155939.txt\n",
            "\n",
            "  0% 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 1/100 [00:00<00:35,  2.76it/s]\u001b[A\n",
            "  2% 2/100 [00:00<00:35,  2.80it/s]\u001b[A\n",
            "  3% 3/100 [00:01<00:34,  2.78it/s]\u001b[A\n",
            "  4% 4/100 [00:01<00:34,  2.80it/s]\u001b[A\n",
            "  5% 5/100 [00:01<00:33,  2.79it/s]\u001b[A\n",
            "  6% 6/100 [00:02<00:33,  2.79it/s]\u001b[A\n",
            "  7% 7/100 [00:02<00:33,  2.79it/s]\u001b[A\n",
            "  8% 8/100 [00:02<00:33,  2.78it/s]\u001b[A\n",
            "  9% 9/100 [00:03<00:33,  2.74it/s]\u001b[A\n",
            " 10% 10/100 [00:03<00:32,  2.76it/s]\u001b[A\n",
            " 11% 11/100 [00:03<00:32,  2.77it/s]\u001b[A\n",
            " 12% 12/100 [00:04<00:31,  2.79it/s]\u001b[A\n",
            " 13% 13/100 [00:04<00:30,  2.82it/s]\u001b[A\n",
            " 14% 14/100 [00:05<00:30,  2.80it/s]\u001b[A\n",
            " 15% 15/100 [00:05<00:29,  2.84it/s]\u001b[A\n",
            " 16% 16/100 [00:05<00:29,  2.82it/s]\u001b[A\n",
            " 17% 17/100 [00:06<00:29,  2.83it/s]\u001b[A\n",
            " 18% 18/100 [00:06<00:28,  2.84it/s]\u001b[A\n",
            " 19% 19/100 [00:06<00:28,  2.84it/s]\u001b[A\n",
            " 20% 20/100 [00:07<00:28,  2.81it/s]\u001b[A\n",
            " 21% 21/100 [00:07<00:27,  2.83it/s]\u001b[A\n",
            " 22% 22/100 [00:07<00:27,  2.83it/s]\u001b[A\n",
            " 23% 23/100 [00:08<00:27,  2.81it/s]\u001b[A\n",
            " 24% 24/100 [00:08<00:26,  2.83it/s]\u001b[A\n",
            " 25% 25/100 [00:08<00:26,  2.82it/s]\u001b[A\n",
            " 26% 26/100 [00:09<00:26,  2.84it/s]\u001b[A\n",
            " 27% 27/100 [00:09<00:25,  2.82it/s]\u001b[A\n",
            " 28% 28/100 [00:09<00:25,  2.80it/s]\u001b[A\n",
            "Loss: 0.409 ‚Äî Avg: 0.408 ‚Äî GPU Mem: 3627 MB: 100% 30000/30000 [3:36:40<00:00,  2.31it/s]\n",
            " 30% 30/100 [00:10<00:24,  2.83it/s]\u001b[A\n",
            " 31% 31/100 [00:11<00:24,  2.84it/s]\u001b[A\n",
            " 32% 32/100 [00:11<00:24,  2.82it/s]\u001b[A\n",
            " 33% 33/100 [00:11<00:23,  2.80it/s]\u001b[A\n",
            " 34% 34/100 [00:12<00:23,  2.78it/s]\u001b[A\n",
            " 35% 35/100 [00:12<00:23,  2.77it/s]\u001b[A\n",
            " 36% 36/100 [00:12<00:22,  2.82it/s]\u001b[A\n",
            " 37% 37/100 [00:13<00:22,  2.83it/s]\u001b[A\n",
            " 38% 38/100 [00:13<00:21,  2.87it/s]\u001b[A\n",
            " 39% 39/100 [00:13<00:21,  2.89it/s]\u001b[A\n",
            " 40% 40/100 [00:14<00:20,  2.90it/s]\u001b[A\n",
            " 41% 41/100 [00:14<00:20,  2.90it/s]\u001b[A\n",
            " 42% 42/100 [00:14<00:20,  2.90it/s]\u001b[A\n",
            " 43% 43/100 [00:15<00:19,  2.89it/s]\u001b[A\n",
            " 44% 44/100 [00:15<00:19,  2.88it/s]\u001b[A\n",
            " 45% 45/100 [00:15<00:19,  2.86it/s]\u001b[A\n",
            " 46% 46/100 [00:16<00:19,  2.84it/s]\u001b[A\n",
            " 47% 47/100 [00:16<00:18,  2.80it/s]\u001b[A\n",
            " 48% 48/100 [00:16<00:17,  3.05it/s]\u001b[A\n",
            " 49% 49/100 [00:17<00:17,  2.97it/s]\u001b[A\n",
            " 50% 50/100 [00:17<00:17,  2.94it/s]\u001b[A\n",
            " 51% 51/100 [00:17<00:16,  2.93it/s]\u001b[A\n",
            " 52% 52/100 [00:18<00:16,  2.91it/s]\u001b[A\n",
            " 53% 53/100 [00:18<00:16,  2.90it/s]\u001b[A\n",
            " 54% 54/100 [00:19<00:15,  2.89it/s]\u001b[A\n",
            " 55% 55/100 [00:19<00:15,  2.90it/s]\u001b[A\n",
            " 56% 56/100 [00:19<00:15,  2.89it/s]\u001b[A\n",
            " 57% 57/100 [00:19<00:11,  3.58it/s]\u001b[A\n",
            " 58% 58/100 [00:20<00:12,  3.29it/s]\u001b[A\n",
            " 59% 59/100 [00:20<00:13,  3.11it/s]\u001b[A\n",
            " 60% 60/100 [00:20<00:13,  2.97it/s]\u001b[A\n",
            " 61% 61/100 [00:21<00:13,  2.88it/s]\u001b[A\n",
            " 62% 62/100 [00:21<00:13,  2.84it/s]\u001b[A\n",
            " 63% 63/100 [00:22<00:13,  2.80it/s]\u001b[A\n",
            " 64% 64/100 [00:22<00:12,  2.79it/s]\u001b[A\n",
            " 65% 65/100 [00:22<00:12,  2.80it/s]\u001b[A\n",
            " 66% 66/100 [00:23<00:12,  2.82it/s]\u001b[A\n",
            " 67% 67/100 [00:23<00:11,  2.82it/s]\u001b[A\n",
            " 68% 68/100 [00:23<00:11,  2.82it/s]\u001b[A\n",
            " 69% 69/100 [00:24<00:10,  2.82it/s]\u001b[A\n",
            " 70% 70/100 [00:24<00:10,  2.87it/s]\u001b[A\n",
            " 71% 71/100 [00:24<00:10,  2.89it/s]\u001b[A\n",
            " 72% 72/100 [00:25<00:09,  2.87it/s]\u001b[A\n",
            " 73% 73/100 [00:25<00:09,  2.87it/s]\u001b[A\n",
            " 74% 74/100 [00:25<00:09,  2.86it/s]\u001b[A\n",
            " 75% 75/100 [00:26<00:08,  2.82it/s]\u001b[A\n",
            " 76% 76/100 [00:26<00:08,  2.83it/s]\u001b[A\n",
            " 77% 77/100 [00:26<00:08,  2.85it/s]\u001b[A\n",
            " 78% 78/100 [00:27<00:07,  2.82it/s]\u001b[A\n",
            " 79% 79/100 [00:27<00:07,  2.83it/s]\u001b[A\n",
            " 80% 80/100 [00:27<00:06,  2.96it/s]\u001b[A\n",
            " 81% 81/100 [00:28<00:06,  2.91it/s]\u001b[A\n",
            " 82% 82/100 [00:28<00:06,  2.89it/s]\u001b[A\n",
            " 83% 83/100 [00:29<00:05,  2.88it/s]\u001b[A\n",
            " 84% 84/100 [00:29<00:05,  2.84it/s]\u001b[A\n",
            " 85% 85/100 [00:29<00:05,  2.83it/s]\u001b[A\n",
            " 86% 86/100 [00:30<00:04,  2.83it/s]\u001b[A\n",
            " 87% 87/100 [00:30<00:04,  2.81it/s]\u001b[A\n",
            " 88% 88/100 [00:30<00:04,  2.78it/s]\u001b[A\n",
            " 89% 89/100 [00:31<00:03,  2.78it/s]\u001b[A\n",
            " 90% 90/100 [00:31<00:03,  2.75it/s]\u001b[A\n",
            " 91% 91/100 [00:31<00:03,  2.74it/s]\u001b[A\n",
            " 92% 92/100 [00:32<00:02,  2.73it/s]\u001b[A\n",
            " 93% 93/100 [00:32<00:02,  2.79it/s]\u001b[A\n",
            " 94% 94/100 [00:32<00:02,  2.83it/s]\u001b[A\n",
            " 95% 95/100 [00:33<00:01,  2.83it/s]\u001b[A\n",
            " 96% 96/100 [00:33<00:01,  2.83it/s]\u001b[A\n",
            " 97% 97/100 [00:34<00:01,  2.81it/s]\u001b[A\n",
            " 98% 98/100 [00:34<00:00,  2.80it/s]\u001b[A\n",
            " 99% 99/100 [00:34<00:00,  2.78it/s]\u001b[A\n",
            "100% 100/100 [00:35<00:00,  2.85it/s]\n",
            "INFO:root:Generation completed!\n",
            "Loss: 0.409 ‚Äî Avg: 0.408 ‚Äî GPU Mem: 3627 MB: 100% 30000/30000 [3:37:05<00:00,  2.30it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a94toYckhHIR"
      },
      "source": [
        "---\n",
        "## 4Ô∏è‚É£ Chat with the model\n",
        "\n",
        "Load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-ivHvsOhaNc",
        "outputId": "4f045668-db11-4383-9f8f-a9d18cb1f3ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from aitextgen import aitextgen\n",
        "from pprint import pprint\n",
        "\n",
        "files = os.listdir(\"./pistoBot/data/models_trained/\")\n",
        "files.remove('.gitkeep')\n",
        "folder_name = files[0]\n",
        "\n",
        "model_path = os.path.join(\".\", \"pistoBot\", \"data\", \"models_trained\", folder_name, \"pytorch_model.bin\")\n",
        "config_path = os.path.join(\".\", \"pistoBot\", \"data\", \"models_trained\", folder_name,\"config.json\")\n",
        "vocab_path = os.path.join(\".\", \"pistoBot\", \"data\", \"models_trained\", folder_name,\"aitextgen-vocab.json\")\n",
        "merges_path = os.path.join(\".\", \"pistoBot\", \"data\", \"models_trained\", folder_name,\"aitextgen-merges.txt\")\n",
        "\n",
        "ai = aitextgen(model=model_path, \n",
        "               config=config_path,\n",
        "               vocab_file=vocab_path,\n",
        "               merges_file=merges_path,\n",
        "               to_gpu=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:aitextgen:Loading GPT-2 model from provided ./pistoBot/data/models_trained/03_gpt2scratch_20201026155939/pytorch_model.bin.\n",
            "INFO:aitextgen:Using a custom tokenizer.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbLNJlQokvYf"
      },
      "source": [
        "### üí¨ Interactive mode\n",
        "> Chat with the model one message at a time\n",
        "\n",
        "- Run the following cell and use the prompt (‚úç) to write your messages\n",
        "- The chats messages will show two tags:\n",
        "    - **[others]** tags: messages wrote by the user\n",
        "    - **[me]** tags: messages generated by the model\n",
        "\n",
        "<br>\n",
        "\n",
        "- Error _Max temperature reached_:\n",
        "    - **Solution**: re-run the cell\n",
        "    - Motivation: under the hood the program increase the _temperature_ value to get a new message that start with \"[me]\" tag. This is done until a max value is reached.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBwemFaak1kV",
        "outputId": "7959d9c0-aea4-4515-b8af-fbac4e70d72b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        }
      },
      "source": [
        "chat = []\n",
        "start_temperature = 0.2\n",
        "max_temperature = 5.0\n",
        "\n",
        "for _ in range(5):\n",
        "    new_line = \"[others] \" + input(\"‚úç\") + '\\n'\n",
        "    chat.append(new_line)\n",
        "    \n",
        "    me_token = False\n",
        "    temperature = start_temperature\n",
        "    input_network = ' '.join(chat)\n",
        "    \n",
        "    while not me_token:\n",
        "        text = ai.generate(prompt=input_network, \n",
        "                           return_as_list=True, \n",
        "                           temperature=temperature)\n",
        "        text = text[0] # batch of 1\n",
        "\n",
        "        text = text.split('\\n')\n",
        "        chat_pos = len(chat)\n",
        "        network_reply = text[chat_pos]\n",
        "\n",
        "        if network_reply.startswith('[me]'):\n",
        "            me_token = True\n",
        "            network_reply = text[chat_pos] + '\\n'\n",
        "            chat.append(network_reply)\n",
        "        else:\n",
        "            if temperature >= max_temperature:\n",
        "                temperature = start_temperature\n",
        "                # raise RuntimeError(\"Max temperature reached\")\n",
        "            temperature += 0.1\n",
        "    # print(f'temperature exit: {temperature}')\n",
        "    print('Chat:')\n",
        "    pprint(chat)\n",
        "    print('---------------------')\n",
        "    "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "‚úçhi\n",
            "Chat:\n",
            "['[others] hi\\n', '[me] Haa\\n']\n",
            "---------------------\n",
            "‚úçbol\n",
            "Chat:\n",
            "['[others] hi\\n', '[me] Haa\\n', '[others] bol\\n', '[me] 1hr aur\\n']\n",
            "---------------------\n",
            "‚úçacha \n",
            "Chat:\n",
            "['[others] hi\\n',\n",
            " '[me] Haa\\n',\n",
            " '[others] bol\\n',\n",
            " '[me] 1hr aur\\n',\n",
            " '[others] acha \\n',\n",
            " '[me] Ho jayega\\n']\n",
            "---------------------\n",
            "‚úçfuck\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-beecb959cf39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         text = ai.generate(prompt=input_network, \n\u001b[1;32m     15\u001b[0m                            \u001b[0mreturn_as_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                            temperature=temperature)\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# batch of 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/aitextgen/aitextgen.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, n, prompt, max_length, temperature, do_sample, return_as_list, seed, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mprompt_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/aitextgen/utils.py\u001b[0m in \u001b[0;36mencode_text\u001b[0;34m(text, tokenizer, device)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}